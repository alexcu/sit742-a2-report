{"cells":[{"cell_type":"markdown","source":["# SIT742 Assignment 2\n\n**Alex Cummaudo &lt;[ca@deakin.edu.au](mailto:ca@deakin.edu.au)&gt;, Jake Renzella &lt;[jake.renzella@deakin.edu.au](mailto:jake.renzella@deakin.edu.au)&gt;**<br>\nStudent IDs 217092024, 217108883 (CloudDeakin Group 35)<br>\nDeakin Software and Technology Innovation Laboratory (DSTIL)<br>\nSchool of Information Technology<br>\nDeakin University, Australia"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Prerequisites\n\nFollow the installation instructions [here](https://gist.github.com/ololobus/4c221a0891775eaa86b0) to install the following:\n\n- Apache Spark 2.1.0\n- Python 2.7\n- Java 1.8\n\nIn addition, the following third-party python packages are installed:\n\n- [GeoIP2](http://geoip2.readthedocs.io/en/latest/) to extrapolate IP information\n- [UserAgents](https://pypi.python.org/pypi/user-agents) to extrapolate User Agent information\n- [Networkx](https://networkx.readthedocs.io) to visualise the findings\n\nThese can be installed using `pip`:\n\n<pre>\n$ pip install geoip2 pyyaml ua-parser user-agents networkx\n</pre>\n\nAlterinatively, attach using the Databricks library importer."],"metadata":{}},{"cell_type":"markdown","source":["## 2. Getting Started\n\n### 2.1. Package Imports\n\nBegin by importing all necessary packages."],"metadata":{}},{"cell_type":"code","source":["import networkx as nx\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\nfrom user_agents import parse as ua_parse\nfrom pyspark.mllib.fpm import FPGrowth"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### 2.2 Loading data from S3 bucket\n\nTo begin with, we need to load the data from an S3 bucket, `sit742-htweblog-gz`. I unzipped the data and compressed it using strong gzip compression from the given zip file, as per:\n\n<pre>\n$ unzip /path/to/HTWebLog.zip\n$ gzip -r /path/to/HTWebLog -9\n</pre>\n\nI referred to [this guide](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#mounting-an-s3-bucket) for assistance with mounting Databricks into the S3 bucket. I refer to the constant `PATH_TO_S3_MOUNT` to refer to the S3 mount in Databricks."],"metadata":{}},{"cell_type":"code","source":["# Define constant to path of log data\nPATH_TO_S3_MOUNT = '/mnt/htweblog'\n\n# Check if we have already mounted our S3 Bucket\nhtweblog_s3_mounted = len(filter(lambda mount: mount.mountPoint == PATH_TO_S3_MOUNT, dbutils.fs.mounts())) == 1\n\nif not htweblog_s3_mounted:\n  # Setup AWS configuration\n  ACCESS_KEY = \"AKIAJDP5QWKSBKP74XUA\"\n  SECRET_KEY = \"9c1/vI3MNniajoK7dH7ko+24Ipr47Q4S4Q5ruO9z\".replace(\"/\", \"%2F\")\n  AWS_BUCKET_NAME = \"sit742-htweblog-gz\"\n\n  # Mount S3 bucket\n  dbutils.fs.mount(\"s3n://%s:%s@%s/\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME), PATH_TO_S3_MOUNT)\n  \n# Show mounted files\ndisplay(dbutils.fs.ls(PATH_TO_S3_MOUNT))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### 2.3. Setting constants\n\nTo begin we need to find where our data is located. I created the constant `SERVER_LOGS_GZIP_FILES` to point to where each log file is. \n\nWe can use a sample of the data using the `SAMPLE_LOGS_GZIP_FILES`, controlling whether to use a sample set (i.e., first 10 log files) using the `USE_SAMPLE_SET` constant, which is `False` if we should use all data (for final submission) or `True` for development and debugging purposes.\n\nIn addition, we will use _[GeoIP2](http://dev.maxmind.com/geoip/)_ to extrapolate information about the client's IP. This requires downloading the [GeoLite2 Cities Database](http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz). This has been downloaded in the S3 Bucket as `GeoLite2-City.mmdb`."],"metadata":{}},{"cell_type":"code","source":["GEOLITE_CITIES_DB_FILE = '/dbfs/' + PATH_TO_S3_MOUNT + '/geolite-db/GeoLite2-City.mmdb'\nSAMPLE_LOGS_GZIP_FILES = PATH_TO_S3_MOUNT + '/sample-set/*.log.gz'\nSERVER_LOGS_GZIP_FILES = PATH_TO_S3_MOUNT + '/*.log.gz'\n# Change this to False if we want to run on the entire dataset, otherwise keep to True for testing/debugging\nUSE_SAMPLE_SET = False"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## 3. Data Acquisition\n\n### 3.1. Extract Data\n\nExtract the files from the `SERVER_LOGS_GZIP_FILES` as an _[Apache Spark RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)_, then [caching](http://spark.apache.org/docs/latest/quick-start.html#caching) it for better performance. Note that we can utilise reading directly from within the gzip, as per the [external datasets](http://spark.apache.org/docs/latest/programming-guide.html#external-datasets) guide."],"metadata":{}},{"cell_type":"code","source":["# If USE_SAMPLE_SET, then read from sample set directory, otherwise use all data\ndata_files = SAMPLE_LOGS_GZIP_FILES if USE_SAMPLE_SET else SERVER_LOGS_GZIP_FILES\nprint(\"Using %s set files at: %s\" % ('sample' if USE_SAMPLE_SET else 'full', data_files))\n# Load in Apache Spark RDD (Resillient Distributed Dataset)\nlogs_rdd = sc.textFile(data_files, use_unicode=False)\n# Caching the data to a cluster-wide in-memory cache\nlogs_rdd.cache()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Extracting the fields from the dataset file (using the `Fields` comment):"],"metadata":{}},{"cell_type":"code","source":["# Strip fields from dataset, underscoring each instead of dasherizing it\nfields  = (logs_rdd\n           .filter(lambda line: line.startswith('#Fields:'))\n           .map(lambda line: line.replace('-', '_'))\n           .first()\n           .split(' ')\n          )[1:]\nfields"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### 3.2. Transform Data\n\nTranform the data by zipping the contents with each value, thereby producing a structured format of the key/value pair of each log entry. Also perform additional transformation on the dataset, such as:\n\n- converting relevant integer strings into actual `int`s,\n- converting the `date` and `time` fields into one `timestamp` field, as a `DateTime` object, and\n- making the `cs(User_Agent)` field a little nicer to work with by changing it to just `user_agent`"],"metadata":{}},{"cell_type":"code","source":["def map_integers(record):\n    \"\"\"Maps integer types in the record from unicode strings\"\"\"\n    record['s_port'] = int(record['s_port'])\n    record['sc_status'] = int(record['sc_status'])\n    record['sc_substatus'] = int(record['sc_substatus'])\n    record['sc_win32_status'] = int(record['sc_win32_status'])\n    record['time_taken'] = int(record['time_taken'])\n    return record\n  \ndef map_timestamp(record):\n    \"\"\"Maps a record's date and time into one timestamp\"\"\"\n    record['timestamp'] = datetime.strptime(record.pop('date') + record.pop('time'), '%Y-%m-%d%H:%M:%S')\n    return record\n\ndef map_user_agent(record):\n    \"\"\"Maps the user agent field to be better used\"\"\"\n    record['user_agent'] = record.pop('cs(User_Agent)')\n    return record\n  \n# Map dataset contents (log lines) as dictionary from fields, then map using additional map functions\ndata = (logs_rdd\n        .filter(lambda line: not line.startswith('#'))\n        .map(lambda line: dict(zip(fields, line.split(' '))))\n        .map(map_integers)\n        .map(map_timestamp)\n        .map(map_user_agent)\n       )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### 3.3 Load Data\n\nLoad the data as a structured [data frame](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) from its RDD, registering the data under the `Log` in-memory table. To load it, we will need to map each record as a [Row](https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html) type."],"metadata":{}},{"cell_type":"code","source":["logs_df = data.toDF()\nlogs_df.registerTempTable(\"Log\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"Log\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE Log\").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 3.4. Additional Data Extraction\n\nWe can extract the distinct user agents from the log and load this into its own table, `UserAgent`."],"metadata":{}},{"cell_type":"code","source":["def map_user_agent_partition(partition):\n    \"\"\"Maps a parition of IP addresses\"\"\"\n    def map_user_agent(user_agent_str):\n        \"\"\"Maps the user agent in a record to extrapolate more specific information about the client platform\"\"\"\n        agent_lookup = ua_parse(user_agent_str)\n        return {\n          'user_agent': user_agent_str,\n          'user_agent_browser_name': agent_lookup.browser.family,\n          'user_agent_browser_version': agent_lookup.browser.version_string,\n          'user_agent_os_name': agent_lookup.os.family,\n          'user_agent_os_version': agent_lookup.os.version_string,\n          'user_agent_device_brand': agent_lookup.device.brand,\n          'user_agent_device_model': agent_lookup.device.model,\n          'user_agent_device_family': agent_lookup.device.family,\n          'user_agent_is_pc': agent_lookup.is_pc,\n          'user_agent_is_smartphone': agent_lookup.is_mobile,\n          'user_agent_is_tablet': agent_lookup.is_tablet,\n          'user_agent_is_bot': agent_lookup.is_bot\n        }\n    return [map_user_agent(record['user_agent']) for record in partition]\n\n# Select all unique user agents from the logs\nuser_agents_rdd = sqlContext.sql('SELECT DISTINCT user_agent FROM Log WHERE user_agent IS NOT NULL').rdd\n# Map user agents them using the mapping function above\nuser_agents_df = user_agents_rdd.mapPartitions(map_user_agent_partition).toDF()\n# Register the dataframe as a table, UserAgent\nuser_agents_df.registerTempTable(\"UserAgent\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"UserAgent\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE UserAgent\").show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Similarly, we can extract all the IP addresses into the `IPAddr` table."],"metadata":{}},{"cell_type":"code","source":["def map_ip_address_partition(partition):\n    \"\"\"Maps a parition of IP addresses\"\"\"\n    # Must re-import geoip as mapping within new context\n    # Refer to: http://stackoverflow.com/a/33755564/519967\n    from geoip2 import database as geoipdb\n    geoip_reader = geoipdb.Reader(GEOLITE_CITIES_DB_FILE)\n    def map_ip_address(ip_address):\n      \"\"\"Maps a single IP address to extrapolate more specific information about the IP\"\"\"\n      try:\n        ip_lookup = geoip_reader.city(ip_address)\n        return {\n          'ip_address': ip_address,\n          'country_code': ip_lookup.country.iso_code,\n          'country_name': ip_lookup.country.name,\n          'state_code': ip_lookup.subdivisions.most_specific.iso_code,\n          'state_name': ip_lookup.subdivisions.most_specific.name,\n          'city_name': ip_lookup.city.name,\n          'lat': ip_lookup.location.latitude,\n          'lng': ip_lookup.location.longitude\n        }\n      except:\n          return None\n    result = [map_ip_address(record['ip']) for record in partition]\n    # Must close reader!\n    geoip_reader.close()\n    return result\n\n# Select all unique user agents from the logs, then map them using the mapping function above\nip_addrs_rdd = sqlContext.sql(\"SELECT DISTINCT c_ip AS ip FROM Log WHERE c_ip IS NOT NULL\").rdd\n# Map using mapPartitions functions, removing those countries we can't find (i.e., private IP address)\nip_address_df = ip_addrs_rdd.mapPartitions(map_ip_address_partition).filter(lambda record: record != None).toDF()\n# Register the dataframe as a table, UserAgent\nip_address_df.registerTempTable(\"IPAddr\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"IPAddr\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE IPAddr\").show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["We can see that data has now be loaded by counting the records of our three tables."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"SELECT COUNT(*) AS count_of_logs         FROM Log\").show()\nsqlContext.sql(\"SELECT COUNT(*) AS count_of_user_agents  FROM UserAgent\").show()\nsqlContext.sql(\"SELECT COUNT(*) AS count_of_ip_addresses FROM IPAddr\").show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## 3.5. Persist Data\n\nAs I am using the free Databricks tier, where the cluster will restart after a few hours inactivity, I persisted the data so that I can work on the assignment over multiple days without re-loading the data. To do this, I persisted the data frames using the following:"],"metadata":{}},{"cell_type":"code","source":["logs_df.write.mode(\"ignore\").saveAsTable(\"Log\")\nip_address_df.write.mode(\"ignore\").saveAsTable(\"IPAddr\")\nuser_agents_df.write.mode(\"ignore\").saveAsTable(\"UserAgent\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Then when I want to work with the data, [cache it](https://docs.databricks.com/spark/latest/sparkr/functions/cacheTable.html) for improved query performance:"],"metadata":{}},{"cell_type":"code","source":["sqlContext.cacheTable(\"IPAddr\")\nsqlContext.cacheTable(\"UserAgent\")\nsqlContext.cacheTable(\"Log\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["# 4. Informational Resource Transaction Extraction\n\n## 4.1. Defining a user session\n\nTo extract session information, we generate a new field, the `session_idenfitier`, which is a hash-delimited concatenated string of the following information:\n\n1. The client's ip, `c_ip`,\n1. The client's specific user agent string, `user_agent`,\n1. The client's session date (the `DATE` of the `timestamp`), and\n1. The client's session hour (extracted using `DATE_FORMAT(timestamp, 'H')`)\n\nWe assume that one session is grouped by every hour on a specific date. We can therefore group the order of our requested URIs by the unique `session_identifier` we have created above.\n\nOne client IP may have multiple users, e.g., an internet caf√© or the hotel lobby. Therefore we must split the client IP into sessions based on user agent. Our limitation here is that there may be two _separate_ users requesting the page with the same user agent at the same IP within the same hour.\n\n## 4.2. Defining which resources to mine\n\nTo ensure that we extract _informational resources_ only, we add the following conditions to our request:\n\n1. The request must return an `ashx` or `aspx` resource, not a media resource (e.g., JavaScript, Cascading Stylesheet, Image file etc.),\n1. The request must not be from the `media`, `layouts` or `sitecore` admin directories as this is non-informational data,\n1. The request must return a `200` response, and must not be a placeholder error page (i.e., `404.aspx` should be removed as this is non-informational)\n\nLowercase all the URIs to prevent case sensitiity (i.e., a user types in `/Home.aspx` vs `/home.aspx`; semantically the same).\n\n## 4.3. Functionalising the query \n\nThis is all constructed for us in the `construct_sql_query` function to keep query selection consistent and reduce duplication.\n\nUsing this function we can:\n\n- compare the requests internally versus externally\n- compare how the top three countries differ in their requests (referencing from Assignment 1 we saw these countries are `Hong Kong`, `USA`, `Australia`)\n- compare how mobile versus tablet versus PC vs bot requests differ"],"metadata":{}},{"cell_type":"code","source":["def construct_sql_query(where = None, join = None):\n  \"\"\" Constructs a consistent SQL query for extracting data from the database\n  \n  Args:\n      where_clause (string): An optional string to add an extra WHERE clause to the query\n      join_clause (string): An optional string to add an extra JOIN clause to the query\n                            that must be in the format `JOIN <Table> ON <Join>`\n  Returns:\n      string: A string to run on the database to extract data\n  \"\"\"\n  standard_query = \"\"\"\n    SELECT \n    -- Session identifier defined as thus:\n    CONCAT(\n      -- [1] The client's IP address\n      l.c_ip, '#', \n      -- [2] The client's user agent string\n      l.user_agent, '#',\n      -- [3] The date of the request\n      DATE(l.timestamp), '#',\n      -- [4] The hour of the request\n      DATE_FORMAT(l.timestamp, 'H')\n    ) AS session_identifier,\n    -- URI stem requested, all lowercase to prevent case sensitvity\n    LCASE(l.cs_uri_stem)\n    FROM Log l\n    -- Add extra JOIN clause\n    {join_clause}\n    WHERE\n      -- [1] ASHX or ASPX requests only to filter out other resources\n      (l.cs_uri_stem LIKE \"%.ashx\" OR l.cs_uri_stem LIKE \"%.aspx\") AND\n      -- [2] Remove media, layout templates or admin sitecore requests (non-informational resources)\n      NOT (l.cs_uri_stem LIKE \"%/~/media%\" OR l.cs_uri_stem LIKE \"%/layouts%\" OR l.cs_uri_stem LIKE \"%/sitecore/%\") AND \n      -- [3] Response codes of 200 and not the x0x pages (e.g. 404.aspx)\n      (l.sc_status = 200 AND l.cs_uri_stem NOT LIKE \"/%0%.aspx\")\n      -- Add extra WHERE clause\n      {where_clause}\n    GROUP BY 1, l.timestamp, l.cs_uri_stem\n    ORDER BY l.timestamp\n  \"\"\"\n  # Add an \"AND\" to the where clause if it exists\n  where_clause = (\"AND %s\" % where) if where is not None else \"\"\n  join_clause = join if join is not None else \"\"\n  formatted_query = standard_query.format(join_clause=join_clause, where_clause=where_clause)\n  # Return the formatted query\n  return formatted_query"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Define IP range string for all INTERNAL requests\ninternal_ip_range_string = '(^127\\.)|(^10\\.)|(^172\\.1[6-9]\\.)|(^172\\.2[0-9]\\.)|(^172\\.3[0-1]\\.)|(^192\\.168\\.)'\n\nsql_queries = {\n  # Internal vs external\n  \"internal_requests\": construct_sql_query(where = \"l.c_ip REGEXP '%s'\" % internal_ip_range_string),\n  \"external_requests\": construct_sql_query(where = \"l.c_ip NOT REGEXP '%s'\" % internal_ip_range_string),\n  # Top three countries\n  \"hk_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'HK'\"),\n  \"us_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'US'\"),\n  \"au_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'AU'\"),\n  # PC vs bots vs tablets vs smartphones\n  \"pc_requests\":         construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_pc\"),\n  \"tablet_requests\":     construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_tablet\"),\n  \"bots_requests\":       construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_bot\"),\n  \"smartphone_requests\": construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_smartphone\")\n}"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Run this as an SQL query under the SQL query context to extract the transactional data we are interested in. Map it into a `tuple` type, representing the `extracted` data as a `(Key, Value)` tuple.\n\nDefine this as a function to allow for multiple queries to be made."],"metadata":{}},{"cell_type":"code","source":["def extract_data(sql_query):\n    \"\"\" Extracts data from the database given the SQL query\n    Args:\n        sql_query (str): a string containing the SQL query used to extract the data.\n    Returns:\n        list<tuple>: a list of all records as a tuple of `(session_identifier, cs_uri_stem)`.\n    \"\"\"\n    return sqlContext.sql(sql_query).rdd.map(lambda record: (record[0], record[1]))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Now extract the data for every `sql_query` in our `sql_queries`:"],"metadata":{}},{"cell_type":"code","source":["# Loop through every query and extract data\nextracted_data = {key: extract_data(sql_query) for key, sql_query in sql_queries.items()}"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["# 5. Training the Model\n\nWe now mine for frequent patterns in our transactions Spark FPGrowth implementation. \n\nTo do this, we group all of the extracted data by the unique `session_identifier` key. The `sessionPair` is a the Key/Value pair whose key is the `session_identifier` and whose value is a unique set of the `cs_uri_stem`s accessed. This becomes our list of transactions.\n\nTo ensure we access multiple hits in a given session, we will show only those patterns with at least 3 hits in the session.\n\nFrom this, we produce a list of `FreqItemset`s representing the frequency pattern of resources from the above transactions extracted, sorted in descending frequency order."],"metadata":{}},{"cell_type":"code","source":["def train_model(extracted, min_support_level = 0.01):\n    \"\"\" Train a model using the Frequency Pattern Growth imported from Spark.\n    \n    Extracts the transactions used to train the model and supply it with a provided minimum \n    support level.\n    \n    Args:\n        extracted (list<tuple>): a list of all extracted records from the database.\n        min_support_level (float): the threshold for a `FreqItemset` to be identified as \n                                   frequent, defaults to `0.01`.\n                                   \n    Returns:\n        list<FreqItemset>: A list of the `FreqItemset` identified sorted by descending\n                           frequency values.\n    \"\"\"\n    transactions = (extracted\n                    # Group by each session id\n                    .groupByKey()\n                    # Extract out a set of each URI hit \n                    .map(lambda sessionPair: set(sessionPair[1]))\n                   )\n    model = FPGrowth.train(transactions, minSupport=min_support_level, numPartitions=6)\n    sorted_itemsets = (model.freqItemsets()\n                       # Only show item sets with 3 or more hits in the set\n                       .filter(lambda itemset: len(itemset.items) >= 3)\n                       # Sort in reverse order by frequencies\n                       .sortBy(lambda itemset: itemset.freq, False)\n                       .collect()\n                      )\n    return sorted_itemsets"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Now train the models and print off each of our `FreqItemset`s. For some requests, we relax the pattern minimum support level to either 75% or 50%, as at a minimum support level is `0.01` retrieves few patterns."],"metadata":{}},{"cell_type":"code","source":["# Set the default and relaxed min support levels\ndefault_min_support_level = 0.01\nrelaxed_min_support_level = { \n  \"hk_requests\": default_min_support_level * 0.5,\n  \"au_requests\": default_min_support_level * 0.75,\n  \"smartphone_requests\": default_min_support_level * 0.5,\n  \"pc_requests\": default_min_support_level * 0.5,\n  \"tablet_requests\": default_min_support_level * 0.5\n}\n# Loop through every extracted data and train using that model\nsorted_itemsets = {\n  key: train_model(data, min_support_level=relaxed_min_support_level.get(key, default_min_support_level)) \n  for key, data in extracted_data.items()\n}\nfor key, itemset in sorted_itemsets.iteritems():\n  print \"%s itemset\" % key\n  print\n  for item in itemset:\n    print item"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["# 6. Visualisation of Model\n\n## 6.1. Visualisation using NetworkX\n\nBelow we visualise how people navigate through the site using a [Multi-Directional Network Graph](https://networkx.github.io/documentation/networkx-1.10/reference/classes.multidigraph.html).\n\nTo prevent excessive amounts of data being plotted, we can use the `frequency_threshold_percentile` variable to change how many FP Itemsets are shown. By default, only the top 25% (those with frequencies above the third percentile) will be plotted to keep the visualisations readable. Not doing so lead to [unreadable graphs](https://i.imgur.com/LddbGNO.png).\n\nThe thick ends of the lines indicate the \"to\" direction (i.e., the line from `home` to `offers` has a thick stub toward `offers`, meaning that users would go from home to offers). The values in between each line indicate the frequency of the pattern."],"metadata":{}},{"cell_type":"code","source":["def create_directed_network_graph(sorted_itemsets, frequency_threshold_percentile=75):\n  \"\"\" Creates a directed network graph of the frequency interaction patterns.\n  \n  Args:\n      sorted_itemsets (list<FreqItemset>): A list of the `FreqItemset` identified \n                                           sorted by descending frequency values.\n                                              \n      frequency_threshold_percentile (int): The value of of the minimum percentile to accept\n                                            when plotting. Defaults to the 75th percentile.\n  \n  Returns:\n      tuple: A tuple containing the `NetworkX.DiGraph` and CSV representation (`string`)\n             of interaction patterns: `(graph, csv)`    \n  \"\"\"  \n  # Declare our new graph\n  graph = nx.MultiDiGraph()\n\n  # Declare an empty dictionary for the edge labels\n  edge_labels = {}\n\n  # CSV to be tabulated in LaTeX\n  csv = \"Sequence,From,To,Frequency\"\n\n  # Work out which frequencies we will plot within our threshold\n  assert frequency_threshold_percentile <= 100 and frequency_threshold_percentile >= 0, \"Threshold must be a percentage between 0 and 1\"\n  highest_frequency = sorted_itemsets[0].freq\n  all_frequencies = np.array([ itemset.freq for itemset in sorted_itemsets ])\n  # Accept the \"top nth\" percentile\n  accepted_minimum_frequency = np.percentile(all_frequencies, frequency_threshold_percentile)\n  # Filter out sorted_frequencies\n  accepted_itemsets = [itemset for itemset in sorted_itemsets if itemset.freq >= accepted_minimum_frequency]\n  \n  # Define a colormap for each sequence\n  cmap = plt.cm.get_cmap('Set1', len(accepted_itemsets))\n  sequence_colors = [colors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n  \n  # Add in each node\n  for sequence, freq_itemset in enumerate(accepted_itemsets):\n    # 'Clean up' the label by removing the '.aspx' and leading forward slash\n    items = [label[1:-5].replace('-', ' ') for label in freq_itemset.items]\n    num_items = len(items)\n    # Define freq\n    freq = freq_itemset.freq\n    # Find the previous and following node in the set\n    for i, item in enumerate(items):\n      node_from, node_to = items[0 + i:2 + i]\n      edge_labels[(node_from, node_to)] = freq\n      # Add to our CSV\n      csv = \"%s\\n%i,%s,%s,%i\" % (csv, sequence, node_from, node_to, freq)\n      label = \"%i/%i\" % (sequence, freq)\n      graph.add_edge(node_from, node_to, weight=freq, label=label, color=sequence_colors[sequence])\n      # Break the loop so we don't go out of range!\n      if num_items - i == 2:\n        break\n\n  # Set up the layout of the graph\n  pos = nx.shell_layout(graph, scale=8)\n\n  # Draw the nodes\n  nx.draw_networkx_nodes(graph, pos, node_size=1000)\n\n  # Draw the edges\n  nx.draw_networkx_edges(graph, pos)\n\n  # Draw the labels\n  nx.draw_networkx_labels(graph, pos, font_size=10, font_family='serif')\n  nx.draw_networkx_edge_labels(graph, pos, font_family='serif', font_size=7, alpha=0.5, edge_labels=edge_labels)\n  \n  return (graph, csv)\n\ndef plot_visualisation(sorted_itemsets, frequency_threshold_percentile =75):\n  \"\"\" Plots the visualisation of a specific set of sorted frequencies\n  \n  Args:\n      sorted_itemsets (tuple): The sorted frequencies to visualise\n      \n      frequency_threshold_percentile (int): The value of of the minimum percentile to accept\n                                            when plotting. Defaults to the 75th percentile.\n\n  \n  Returns:\n      tuple: A tuple containing the `NetworkX.DiGraph` and CSV representation (`string`)\n             of interaction patterns: `(graph, csv)`  \n  \"\"\"\n  # Clear last plotted functions\n  plt.clf()\n\n  graph, csv = create_directed_network_graph(sorted_itemsets, frequency_threshold_percentile)\n\n  # Disable the axis and plot\n  plt.axis('off')\n  display(plt.show())\n  \n  return (graph, csv)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["We can now call our function to visualise our respective graphs.\n\n### 6.1.1. Internal Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data = {}\ngraph_data[\"internal_requests\"] = plot_visualisation(sorted_itemsets[\"internal_requests\"])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### 6.1.2. External Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"external_requests\"] = plot_visualisation(sorted_itemsets[\"external_requests\"])"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### 6.1.3. Hong Kong Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"hk_requests\"] = plot_visualisation(sorted_itemsets[\"hk_requests\"])"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### 6.1.4. USA Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"us_requests\"] = plot_visualisation(sorted_itemsets[\"us_requests\"])"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### 6.1.5. Australian Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"au_requests\"] = plot_visualisation(sorted_itemsets[\"au_requests\"])"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### 6.1.6. PC Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"pc_requests\"] = plot_visualisation(sorted_itemsets[\"pc_requests\"])"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### 6.1.7. Smartphone Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"smartphone_requests\"] = plot_visualisation(sorted_itemsets[\"smartphone_requests\"])"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["### 6.1.8. Tablet Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"tablet_requests\"] = plot_visualisation(sorted_itemsets[\"tablet_requests\"])"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["### 6.1.9. Bot Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"bots_requests\"] = plot_visualisation(sorted_itemsets[\"bots_requests\"])"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["## 6.2. Improved plotting using GraphViz\n\nHowever, the above is hard to read, especially the frequency values. We can convert the graph into a [Graphviz](http://www.graphviz.org) diagram string. Install the dependency as needed:\n\n```\n$ brew install graphviz\n```\n\nRunning the command below, we can copy the output and run through the `dot` command provided by Graphviz:\n\n```\n$ pbpaste > a2.dot\n$ dot internal_requests.dot -T pdf > internal_requests.pdf\n```"],"metadata":{}},{"cell_type":"code","source":["def graph_to_pydot_string(graph, layout=\"dot\"):\n  \"\"\" Converts the graph to a representable Graphviz diagram using pydot\n  \n  Args:\n      graph (`NetworkX.DiGraph`): The graph to convert\n  \n  Returns:\n      string: A string representing the Graphviz diagram string with the \n              layout specified, defaults to `dot`.\n  \"\"\"\n  string = nx.drawing.nx_pydot.to_pydot(graph).to_string()\n  # Split all lines to add the specified layout\n  lines = string.split(\"\\n\")\n  lines.insert(1, 'layout=\"%s\";' % layout)\n  return \"\\n\".join(lines)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["### 6.2.1. Internal Requests\n\nNow run our conversion function on our graph:"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"internal_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Save the output above to file `internal_requests.dot` and convert using the commands described above.\n\nThis produces a much cleaner looking output, where edges are colorised for assisting with reading frequency patterns between pages.\n\n![Dot output](https://i.imgur.com/hbxRIYj.png)"],"metadata":{}},{"cell_type":"markdown","source":["### 6.2.2. External Requests\n\n![Dot image](https://i.imgur.com/3wco2n9.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"external_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["### 6.2.3. Hong Kong Requests\n\n![Dot image](https://i.imgur.com/7KoDjBd.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"hk_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["### 6.2.4. USA Requests\n\n![Dot image](https://i.imgur.com/8qvRlq1.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"us_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["### 6.2.5. Australian Requests\n\n![Dot image](https://i.imgur.com/4uvAdI9.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"au_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["### 6.2.6. PC Requests\n\n![Dot image](https://i.imgur.com/3jPUfDZ.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"pc_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["### 6.2.7. Smartphone Requests\n\n![Dot image](https://i.imgur.com/Ne5cq0l.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"smartphone_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["### 6.2.8. Tablet Requests\n\n![Dot image](https://i.imgur.com/kWvmh2l.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"tablet_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["### 6.2.9. Bot Requests\n\n![Bot requests](https://i.imgur.com/RZB4Ab7.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"bots_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["## 7. CSV Output\n\nWe can provide a reference table to support the diagram as a table and import this into our LaTeX report:"],"metadata":{}},{"cell_type":"code","source":["print \"internal_requests.csv\"\nprint graph_data[\"internal_requests\"][1]\nprint\nprint \"external_requests.csv\"\nprint graph_data[\"external_requests\"][1]\nprint\nprint \"hk_requests.csv\"\nprint graph_data[\"hk_requests\"][1]\nprint\nprint \"us_requests.csv\"\nprint graph_data[\"us_requests\"][1]\nprint\nprint \"au_requests.csv\"\nprint graph_data[\"au_requests\"][1]\nprint\nprint \"pc_requests.csv\"\nprint graph_data[\"pc_requests\"][1]\nprint\nprint \"smartphone_requests.csv\"\nprint graph_data[\"smartphone_requests\"][1]\nprint\nprint \"tablet_requests.csv\"\nprint graph_data[\"tablet_requests\"][1]\nprint\nprint \"bots_requests.csv\"\nprint graph_data[\"bots_requests\"][1]"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["print \"internal_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"internal_requests\"][0])\nprint\nprint \"external_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"external_requests\"][0])\nprint\nprint \"hk_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"hk_requests\"][0])\nprint\nprint \"us_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"us_requests\"][0])\nprint\nprint \"au_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"au_requests\"][0])\nprint\nprint \"pc_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"pc_requests\"][0])\nprint\nprint \"smartphone_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"smartphone_requests\"][0])\nprint\nprint \"tablet_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"tablet_requests\"][0])\nprint\nprint \"bots_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"bots_requests\"][0])\nprint"],"metadata":{},"outputs":[],"execution_count":81}],"metadata":{"name":"DataAnalytics","notebookId":980727703319900},"nbformat":4,"nbformat_minor":0}
