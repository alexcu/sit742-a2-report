{"cells":[{"cell_type":"markdown","source":["# SIT742 Assignment 2\n\n**Alex Cummaudo &lt;[ca@deakin.edu.au](mailto:ca@deakin.edu.au)&gt;, Jake Renzella &lt;[jake.renzella@deakin.edu.au](mailto:jake.renzella@deakin.edu.au)&gt;**<br>\nStudent IDs 217092024, 217108883 (CloudDeakin Group 35)<br>\nDeakin Software and Technology Innovation Laboratory (DSTIL)<br>\nSchool of Information Technology<br>\nDeakin University, Australia"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Prerequisites\n\nFollow the installation instructions [here](https://gist.github.com/ololobus/4c221a0891775eaa86b0) to install the following:\n\n- Apache Spark 2.1.0\n- Python 2.7\n- Java 1.8\n\nIn addition, the following third-party python packages are installed:\n\n- [GeoIP2](http://geoip2.readthedocs.io/en/latest/) to extrapolate IP information\n- [UserAgents](https://pypi.python.org/pypi/user-agents) to extrapolate User Agent information\n- [Networkx](https://networkx.readthedocs.io) to visualise the findings\n\nThese can be installed using `pip`:\n\n<pre>\n$ pip install geoip2 pyyaml ua-parser user-agents networkx\n</pre>\n\nAlterinatively, attach using the Databricks library importer."],"metadata":{}},{"cell_type":"markdown","source":["## 2. Getting Started\n\n### 2.1. Package Imports\n\nBegin by importing all necessary packages."],"metadata":{}},{"cell_type":"code","source":["import networkx as nx\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\nfrom user_agents import parse as ua_parse\nfrom pyspark.mllib.fpm import FPGrowth"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### 2.2 Loading data from S3 bucket\n\nTo begin with, we need to load the data from an S3 bucket, `sit742-htweblog-gz`. I unzipped the data and compressed it using strong gzip compression from the given zip file, as per:\n\n<pre>\n$ unzip /path/to/HTWebLog.zip\n$ gzip -r /path/to/HTWebLog -9\n</pre>\n\nI referred to [this guide](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#mounting-an-s3-bucket) for assistance with mounting Databricks into the S3 bucket. I refer to the constant `PATH_TO_S3_MOUNT` to refer to the S3 mount in Databricks."],"metadata":{}},{"cell_type":"code","source":["# Define constant to path of log data\nPATH_TO_S3_MOUNT = '/mnt/htweblog'\n\n# Check if we have already mounted our S3 Bucket\nhtweblog_s3_mounted = len(filter(lambda mount: mount.mountPoint == PATH_TO_S3_MOUNT, dbutils.fs.mounts())) == 1\n\nif not htweblog_s3_mounted:\n  # Setup AWS configuration\n  ACCESS_KEY = \"AKIAJDP5QWKSBKP74XUA\"\n  SECRET_KEY = \"9c1/vI3MNniajoK7dH7ko+24Ipr47Q4S4Q5ruO9z\".replace(\"/\", \"%2F\")\n  AWS_BUCKET_NAME = \"sit742-htweblog-gz\"\n\n  # Mount S3 bucket\n  dbutils.fs.mount(\"s3n://%s:%s@%s/\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME), PATH_TO_S3_MOUNT)\n  \n# Show mounted files\ndisplay(dbutils.fs.ls(PATH_TO_S3_MOUNT))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### 2.3. Setting constants\n\nTo begin we need to find where our data is located. I created the constant `SERVER_LOGS_GZIP_FILES` to point to where each log file is. \n\nWe can use a sample of the data using the `SAMPLE_LOGS_GZIP_FILES`, controlling whether to use a sample set (i.e., first 10 log files) using the `USE_SAMPLE_SET` constant, which is `False` if we should use all data (for final submission) or `True` for development and debugging purposes.\n\nIn addition, we will use _[GeoIP2](http://dev.maxmind.com/geoip/)_ to extrapolate information about the client's IP. This requires downloading the [GeoLite2 Cities Database](http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz). This has been downloaded in the S3 Bucket as `GeoLite2-City.mmdb`."],"metadata":{}},{"cell_type":"code","source":["GEOLITE_CITIES_DB_FILE = '/dbfs/' + PATH_TO_S3_MOUNT + '/geolite-db/GeoLite2-City.mmdb'\nSAMPLE_LOGS_GZIP_FILES = PATH_TO_S3_MOUNT + '/sample-set/*.log.gz'\nSERVER_LOGS_GZIP_FILES = PATH_TO_S3_MOUNT + '/*.log.gz'\n# Change this to False if we want to run on the entire dataset, otherwise keep to True for testing/debugging\nUSE_SAMPLE_SET = False"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## 3. Data Acquisition\n\n### 3.1. Extract Data\n\nExtract the files from the `SERVER_LOGS_GZIP_FILES` as an _[Apache Spark RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)_, then [caching](http://spark.apache.org/docs/latest/quick-start.html#caching) it for better performance. Note that we can utilise reading directly from within the gzip, as per the [external datasets](http://spark.apache.org/docs/latest/programming-guide.html#external-datasets) guide."],"metadata":{}},{"cell_type":"code","source":["# If USE_SAMPLE_SET, then read from sample set directory, otherwise use all data\ndata_files = SAMPLE_LOGS_GZIP_FILES if USE_SAMPLE_SET else SERVER_LOGS_GZIP_FILES\nprint(\"Using %s set files at: %s\" % ('sample' if USE_SAMPLE_SET else 'full', data_files))\n# Load in Apache Spark RDD (Resillient Distributed Dataset)\nlogs_rdd = sc.textFile(data_files, use_unicode=False)\n# Caching the data to a cluster-wide in-memory cache\nlogs_rdd.cache()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Extracting the fields from the dataset file (using the `Fields` comment):"],"metadata":{}},{"cell_type":"code","source":["# Strip fields from dataset, underscoring each instead of dasherizing it\nfields  = (logs_rdd\n           .filter(lambda line: line.startswith('#Fields:'))\n           .map(lambda line: line.replace('-', '_'))\n           .first()\n           .split(' ')\n          )[1:]\nfields"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### 3.2. Transform Data\n\nTranform the data by zipping the contents with each value, thereby producing a structured format of the key/value pair of each log entry. Also perform additional transformation on the dataset, such as:\n\n- converting relevant integer strings into actual `int`s,\n- converting the `date` and `time` fields into one `timestamp` field, as a `DateTime` object, and\n- making the `cs(User_Agent)` field a little nicer to work with by changing it to just `user_agent`"],"metadata":{}},{"cell_type":"code","source":["def map_integers(record):\n    \"\"\"Maps integer types in the record from unicode strings\"\"\"\n    record['s_port'] = int(record['s_port'])\n    record['sc_status'] = int(record['sc_status'])\n    record['sc_substatus'] = int(record['sc_substatus'])\n    record['sc_win32_status'] = int(record['sc_win32_status'])\n    record['time_taken'] = int(record['time_taken'])\n    return record\n  \ndef map_timestamp(record):\n    \"\"\"Maps a record's date and time into one timestamp\"\"\"\n    record['timestamp'] = datetime.strptime(record.pop('date') + record.pop('time'), '%Y-%m-%d%H:%M:%S')\n    return record\n\ndef map_user_agent(record):\n    \"\"\"Maps the user agent field to be better used\"\"\"\n    record['user_agent'] = record.pop('cs(User_Agent)')\n    return record\n  \n# Map dataset contents (log lines) as dictionary from fields, then map using additional map functions\ndata = (logs_rdd\n        .filter(lambda line: not line.startswith('#'))\n        .map(lambda line: dict(zip(fields, line.split(' '))))\n        .map(map_integers)\n        .map(map_timestamp)\n        .map(map_user_agent)\n       )"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### 3.3 Load Data\n\nLoad the data as a structured [data frame](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) from its RDD, registering the data under the `Log` in-memory table. To load it, we will need to map each record as a [Row](https://spark.apache.org/docs/1.1.1/api/python/pyspark.sql.Row-class.html) type."],"metadata":{}},{"cell_type":"code","source":["logs_df = data.toDF()\nlogs_df.registerTempTable(\"Log\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"Log\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE Log\").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### 3.4. Additional Data Extraction\n\nWe can extract the distinct user agents from the log and load this into its own table, `UserAgent`."],"metadata":{}},{"cell_type":"code","source":["def map_user_agent_partition(partition):\n    \"\"\"Maps a parition of IP addresses\"\"\"\n    def map_user_agent(user_agent_str):\n        \"\"\"Maps the user agent in a record to extrapolate more specific information about the client platform\"\"\"\n        agent_lookup = ua_parse(user_agent_str)\n        return {\n          'user_agent': user_agent_str,\n          'user_agent_browser_name': agent_lookup.browser.family,\n          'user_agent_browser_version': agent_lookup.browser.version_string,\n          'user_agent_os_name': agent_lookup.os.family,\n          'user_agent_os_version': agent_lookup.os.version_string,\n          'user_agent_device_brand': agent_lookup.device.brand,\n          'user_agent_device_model': agent_lookup.device.model,\n          'user_agent_device_family': agent_lookup.device.family,\n          'user_agent_is_pc': agent_lookup.is_pc,\n          'user_agent_is_smartphone': agent_lookup.is_mobile,\n          'user_agent_is_tablet': agent_lookup.is_tablet,\n          'user_agent_is_bot': agent_lookup.is_bot\n        }\n    return [map_user_agent(record['user_agent']) for record in partition]\n\n# Select all unique user agents from the logs\nuser_agents_rdd = sqlContext.sql('SELECT DISTINCT user_agent FROM Log WHERE user_agent IS NOT NULL').rdd\n# Map user agents them using the mapping function above\nuser_agents_df = user_agents_rdd.mapPartitions(map_user_agent_partition).toDF()\n# Register the dataframe as a table, UserAgent\nuser_agents_df.registerTempTable(\"UserAgent\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"UserAgent\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE UserAgent\").show()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Similarly, we can extract all the IP addresses into the `IPAddr` table."],"metadata":{}},{"cell_type":"code","source":["def map_ip_address_partition(partition):\n    \"\"\"Maps a parition of IP addresses\"\"\"\n    # Must re-import geoip as mapping within new context\n    # Refer to: http://stackoverflow.com/a/33755564/519967\n    from geoip2 import database as geoipdb\n    geoip_reader = geoipdb.Reader(GEOLITE_CITIES_DB_FILE)\n    def map_ip_address(ip_address):\n      \"\"\"Maps a single IP address to extrapolate more specific information about the IP\"\"\"\n      try:\n        ip_lookup = geoip_reader.city(ip_address)\n        return {\n          'ip_address': ip_address,\n          'country_code': ip_lookup.country.iso_code,\n          'country_name': ip_lookup.country.name,\n          'state_code': ip_lookup.subdivisions.most_specific.iso_code,\n          'state_name': ip_lookup.subdivisions.most_specific.name,\n          'city_name': ip_lookup.city.name,\n          'lat': ip_lookup.location.latitude,\n          'lng': ip_lookup.location.longitude\n        }\n      except:\n          return None\n    result = [map_ip_address(record['ip']) for record in partition]\n    # Must close reader!\n    geoip_reader.close()\n    return result\n\n# Select all unique user agents from the logs, then map them using the mapping function above\nip_addrs_rdd = sqlContext.sql(\"SELECT DISTINCT c_ip AS ip FROM Log WHERE c_ip IS NOT NULL\").rdd\n# Map using mapPartitions functions, removing those countries we can't find (i.e., private IP address)\nip_address_df = ip_addrs_rdd.mapPartitions(map_ip_address_partition).filter(lambda record: record != None).toDF()\n# Register the dataframe as a table, UserAgent\nip_address_df.registerTempTable(\"IPAddr\")\n# Cache the table for improved performance\nsqlContext.cacheTable(\"IPAddr\")\n# Show that the table has been registered\nsqlContext.sql(\"DESCRIBE TABLE IPAddr\").show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["We can see that data has now be loaded by counting the records of our three tables."],"metadata":{}},{"cell_type":"code","source":["sqlContext.sql(\"SELECT COUNT(*) AS count_of_logs         FROM Log\").show()\nsqlContext.sql(\"SELECT COUNT(*) AS count_of_user_agents  FROM UserAgent\").show()\nsqlContext.sql(\"SELECT COUNT(*) AS count_of_ip_addresses FROM IPAddr\").show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## 3.5. Persist Data\n\nAs I am using the free Databricks tier, where the cluster will restart after a few hours inactivity, I persisted the data so that I can work on the assignment over multiple days without re-loading the data. To do this, I persisted the data frames using the following:"],"metadata":{}},{"cell_type":"code","source":["logs_df.write.mode(\"ignore\").saveAsTable(\"Log\")\nip_address_df.write.mode(\"ignore\").saveAsTable(\"IPAddr\")\nuser_agents_df.write.mode(\"ignore\").saveAsTable(\"UserAgent\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Then when I want to work with the data, [cache it](https://docs.databricks.com/spark/latest/sparkr/functions/cacheTable.html) for improved query performance:"],"metadata":{}},{"cell_type":"code","source":["sqlContext.cacheTable(\"IPAddr\")\nsqlContext.cacheTable(\"UserAgent\")\nsqlContext.cacheTable(\"Log\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["# 4. Informational Resource Transaction Extraction\n\n## 4.1. Defining a user session\n\nTo extract session information, we generate a new field, the `session_idenfitier`, which is a hash-delimited concatenated string of the following information:\n\n1. The client's ip, `c_ip`,\n1. The client's specific user agent string, `user_agent`,\n1. The client's session date (the `DATE` of the `timestamp`), and\n1. The client's session hour (extracted using `DATE_FORMAT(timestamp, 'H')`)\n\nWe assume that one session is grouped by every hour on a specific date. We can therefore group the order of our requested URIs by the unique `session_identifier` we have created above.\n\nOne client IP may have multiple users, e.g., an internet café or the hotel lobby. Therefore we must split the client IP into sessions based on user agent. Our limitation here is that there may be two _separate_ users requesting the page with the same user agent at the same IP within the same hour.\n\n## 4.2. Defining which resources to mine\n\nTo ensure that we extract _informational resources_ only, we add the following conditions to our request:\n\n1. The request must return an `ashx` or `aspx` resource, not a media resource (e.g., JavaScript, Cascading Stylesheet, Image file etc.),\n1. The request must not be from the `media`, `layouts` or `sitecore` admin directories as this is non-informational data,\n1. The request must return a `200` response, and must not be a placeholder error page (i.e., `404.aspx` should be removed as this is non-informational)\n\nLowercase all the URIs to prevent case sensitiity (i.e., a user types in `/Home.aspx` vs `/home.aspx`; semantically the same).\n\n## 4.3. Functionalising the query \n\nThis is all constructed for us in the `construct_sql_query` function to keep query selection consistent and reduce duplication.\n\nUsing this function we can:\n\n- compare the requests internally versus externally\n- compare how the top three countries differ in their requests (referencing from Assignment 1 we saw these countries are `Hong Kong`, `USA`, `Australia`)\n- compare how mobile versus tablet versus PC vs bot requests differ"],"metadata":{}},{"cell_type":"code","source":["def construct_sql_query(where = None, join = None):\n  \"\"\" Constructs a consistent SQL query for extracting data from the database\n  \n  Args:\n      where_clause (string): An optional string to add an extra WHERE clause to the query\n      join_clause (string): An optional string to add an extra JOIN clause to the query\n                            that must be in the format `JOIN <Table> ON <Join>`\n  Returns:\n      string: A string to run on the database to extract data\n  \"\"\"\n  standard_query = \"\"\"\n    SELECT \n    -- Session identifier defined as thus:\n    CONCAT(\n      -- [1] The client's IP address\n      l.c_ip, '#', \n      -- [2] The client's user agent string\n      l.user_agent, '#',\n      -- [3] The date of the request\n      DATE(l.timestamp), '#',\n      -- [4] The hour of the request\n      DATE_FORMAT(l.timestamp, 'H')\n    ) AS session_identifier,\n    -- URI stem requested, all lowercase to prevent case sensitvity\n    LCASE(l.cs_uri_stem)\n    FROM Log l\n    -- Add extra JOIN clause\n    {join_clause}\n    WHERE\n      -- [1] ASHX or ASPX requests only to filter out other resources\n      (l.cs_uri_stem LIKE \"%.ashx\" OR l.cs_uri_stem LIKE \"%.aspx\") AND\n      -- [2] Remove media, layout templates or admin sitecore requests (non-informational resources)\n      NOT (l.cs_uri_stem LIKE \"%/~/media%\" OR l.cs_uri_stem LIKE \"%/layouts%\" OR l.cs_uri_stem LIKE \"%/sitecore/%\") AND \n      -- [3] Response codes of 200 and not the x0x pages (e.g. 404.aspx)\n      (l.sc_status = 200 AND l.cs_uri_stem NOT LIKE \"/%0%.aspx\")\n      -- Add extra WHERE clause\n      {where_clause}\n    GROUP BY 1, l.timestamp, l.cs_uri_stem\n    ORDER BY l.timestamp\n  \"\"\"\n  # Add an \"AND\" to the where clause if it exists\n  where_clause = (\"AND %s\" % where) if where is not None else \"\"\n  join_clause = join if join is not None else \"\"\n  formatted_query = standard_query.format(join_clause=join_clause, where_clause=where_clause)\n  # Return the formatted query\n  return formatted_query"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Define IP range string for all INTERNAL requests\ninternal_ip_range_string = '(^127\\.)|(^10\\.)|(^172\\.1[6-9]\\.)|(^172\\.2[0-9]\\.)|(^172\\.3[0-1]\\.)|(^192\\.168\\.)'\n\nsql_queries = {\n  # Internal vs external\n  \"internal_requests\": construct_sql_query(where = \"l.c_ip REGEXP '%s'\" % internal_ip_range_string),\n  \"external_requests\": construct_sql_query(where = \"l.c_ip NOT REGEXP '%s'\" % internal_ip_range_string),\n  # Top three countries\n  \"hk_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'HK'\"),\n  \"us_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'US'\"),\n  \"au_requests\": construct_sql_query(join = \"JOIN IPAddr i ON l.c_ip = i.ip_address\", where=\"i.country_code = 'AU'\"),\n  # PC vs bots vs tablets vs smartphones\n  \"pc_requests\":         construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_pc\"),\n  \"tablet_requests\":     construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_tablet\"),\n  \"bots_requests\":       construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_bot\"),\n  \"smartphone_requests\": construct_sql_query(join = \"JOIN UserAgent ua ON ua.user_agent = l.user_agent\", where=\"ua.user_agent_is_smartphone\")\n}"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Run this as an SQL query under the SQL query context to extract the transactional data we are interested in. Map it into a `tuple` type, representing the `extracted` data as a `(Key, Value)` tuple.\n\nDefine this as a function to allow for multiple queries to be made."],"metadata":{}},{"cell_type":"code","source":["def extract_data(sql_query):\n    \"\"\" Extracts data from the database given the SQL query\n    Args:\n        sql_query (str): a string containing the SQL query used to extract the data.\n    Returns:\n        list<tuple>: a list of all records as a tuple of `(session_identifier, cs_uri_stem)`.\n    \"\"\"\n    return sqlContext.sql(sql_query).rdd.map(lambda record: (record[0], record[1]))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Now extract the data for every `sql_query` in our `sql_queries`:"],"metadata":{}},{"cell_type":"code","source":["# Loop through every query and extract data\nextracted_data = {key: extract_data(sql_query) for key, sql_query in sql_queries.items()}"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["# 5. Training the Model\n\nWe now mine for frequent patterns in our transactions Spark FPGrowth implementation. \n\nTo do this, we group all of the extracted data by the unique `session_identifier` key. The `sessionPair` is a the Key/Value pair whose key is the `session_identifier` and whose value is a unique set of the `cs_uri_stem`s accessed. This becomes our list of transactions.\n\nTo ensure we access multiple hits in a given session, we will show only those patterns with at least 3 hits in the session.\n\nFrom this, we produce a list of `FreqItemset`s representing the frequency pattern of resources from the above transactions extracted, sorted in descending frequency order."],"metadata":{}},{"cell_type":"code","source":["def train_model(extracted, min_support_level = 0.01):\n    \"\"\" Train a model using the Frequency Pattern Growth imported from Spark.\n    \n    Extracts the transactions used to train the model and supply it with a provided minimum \n    support level.\n    \n    Args:\n        extracted (list<tuple>): a list of all extracted records from the database.\n        min_support_level (float): the threshold for a `FreqItemset` to be identified as \n                                   frequent, defaults to `0.01`.\n                                   \n    Returns:\n        list<FreqItemset>: A list of the `FreqItemset` identified sorted by descending\n                           frequency values.\n    \"\"\"\n    transactions = (extracted\n                    # Group by each session id\n                    .groupByKey()\n                    # Extract out a set of each URI hit \n                    .map(lambda sessionPair: set(sessionPair[1]))\n                   )\n    model = FPGrowth.train(transactions, minSupport=min_support_level, numPartitions=6)\n    sorted_itemsets = (model.freqItemsets()\n                       # Only show item sets with 3 or more hits in the set\n                       .filter(lambda itemset: len(itemset.items) >= 3)\n                       # Sort in reverse order by frequencies\n                       .sortBy(lambda itemset: itemset.freq, False)\n                       .collect()\n                      )\n    return sorted_itemsets"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Now train the models and print off each of our `FreqItemset`s. For some requests, we relax the pattern minimum support level to either 75% or 50%, as at a minimum support level is `0.01` retrieves few patterns."],"metadata":{}},{"cell_type":"code","source":["# Set the default and relaxed min support levels\ndefault_min_support_level = 0.01\nrelaxed_min_support_level = { \n  \"hk_requests\": default_min_support_level * 0.5,\n  \"au_requests\": default_min_support_level * 0.75,\n  \"smartphone_requests\": default_min_support_level * 0.5,\n  \"pc_requests\": default_min_support_level * 0.5,\n  \"tablet_requests\": default_min_support_level * 0.5\n}\n# Loop through every extracted data and train using that model\nsorted_itemsets = {\n  key: train_model(data, min_support_level=relaxed_min_support_level.get(key, default_min_support_level)) \n  for key, data in extracted_data.items()\n}\nfor key, itemset in sorted_itemsets.iteritems():\n  print \"%s itemset\" % key\n  print\n  for item in itemset:\n    print item"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["# 6. Visualisation of Model\n\n## 6.1. Visualisation using NetworkX\n\nBelow we visualise how people navigate through the site using a [Multi-Directional Network Graph](https://networkx.github.io/documentation/networkx-1.10/reference/classes.multidigraph.html).\n\nTo prevent excessive amounts of data being plotted, we can use the `frequency_threshold_percentile` variable to change how many FP Itemsets are shown. By default, only the top 25% (those with frequencies above the third percentile) will be plotted to keep the visualisations readable. Not doing so lead to [unreadable graphs](https://i.imgur.com/LddbGNO.png).\n\nThe thick ends of the lines indicate the \"to\" direction (i.e., the line from `home` to `offers` has a thick stub toward `offers`, meaning that users would go from home to offers). The values in between each line indicate the frequency of the pattern."],"metadata":{}},{"cell_type":"code","source":["def create_directed_network_graph(sorted_itemsets, frequency_threshold_percentile=75):\n  \"\"\" Creates a directed network graph of the frequency interaction patterns.\n  \n  Args:\n      sorted_itemsets (list<FreqItemset>): A list of the `FreqItemset` identified \n                                           sorted by descending frequency values.\n                                              \n      frequency_threshold_percentile (int): The value of of the minimum percentile to accept\n                                            when plotting. Defaults to the 75th percentile.\n  \n  Returns:\n      tuple: A tuple containing the `NetworkX.DiGraph` and CSV representation (`string`)\n             of interaction patterns: `(graph, csv)`    \n  \"\"\"  \n  # Declare our new graph\n  graph = nx.MultiDiGraph()\n\n  # Declare an empty dictionary for the edge labels\n  edge_labels = {}\n\n  # CSV to be tabulated in LaTeX\n  csv = \"Sequence,From,To,Frequency\"\n\n  # Work out which frequencies we will plot within our threshold\n  assert frequency_threshold_percentile <= 100 and frequency_threshold_percentile >= 0, \"Threshold must be a percentage between 0 and 1\"\n  highest_frequency = sorted_itemsets[0].freq\n  all_frequencies = np.array([ itemset.freq for itemset in sorted_itemsets ])\n  # Accept the \"top nth\" percentile\n  accepted_minimum_frequency = np.percentile(all_frequencies, frequency_threshold_percentile)\n  # Filter out sorted_frequencies\n  accepted_itemsets = [itemset for itemset in sorted_itemsets if itemset.freq >= accepted_minimum_frequency]\n  \n  # Define a colormap for each sequence\n  cmap = plt.cm.get_cmap('Set1', len(accepted_itemsets))\n  sequence_colors = [colors.rgb2hex(cmap(i)[:3]) for i in range(cmap.N)]\n  \n  # Add in each node\n  for sequence, freq_itemset in enumerate(accepted_itemsets):\n    # 'Clean up' the label by removing the '.aspx' and leading forward slash\n    items = [label[1:-5].replace('-', ' ') for label in freq_itemset.items]\n    num_items = len(items)\n    # Define freq\n    freq = freq_itemset.freq\n    # Find the previous and following node in the set\n    for i, item in enumerate(items):\n      node_from, node_to = items[0 + i:2 + i]\n      edge_labels[(node_from, node_to)] = freq\n      # Add to our CSV\n      csv = \"%s\\n%i,%s,%s,%i\" % (csv, sequence, node_from, node_to, freq)\n      label = \"%i/%i\" % (sequence, freq)\n      graph.add_edge(node_from, node_to, weight=freq, label=label, color=sequence_colors[sequence])\n      # Break the loop so we don't go out of range!\n      if num_items - i == 2:\n        break\n\n  # Set up the layout of the graph\n  pos = nx.shell_layout(graph, scale=8)\n\n  # Draw the nodes\n  nx.draw_networkx_nodes(graph, pos, node_size=1000)\n\n  # Draw the edges\n  nx.draw_networkx_edges(graph, pos)\n\n  # Draw the labels\n  nx.draw_networkx_labels(graph, pos, font_size=10, font_family='serif')\n  nx.draw_networkx_edge_labels(graph, pos, font_family='serif', font_size=7, alpha=0.5, edge_labels=edge_labels)\n  \n  return (graph, csv)\n\ndef plot_visualisation(sorted_itemsets, frequency_threshold_percentile =75):\n  \"\"\" Plots the visualisation of a specific set of sorted frequencies\n  \n  Args:\n      sorted_itemsets (tuple): The sorted frequencies to visualise\n      \n      frequency_threshold_percentile (int): The value of of the minimum percentile to accept\n                                            when plotting. Defaults to the 75th percentile.\n\n  \n  Returns:\n      tuple: A tuple containing the `NetworkX.DiGraph` and CSV representation (`string`)\n             of interaction patterns: `(graph, csv)`  \n  \"\"\"\n  # Clear last plotted functions\n  plt.clf()\n\n  graph, csv = create_directed_network_graph(sorted_itemsets, frequency_threshold_percentile)\n\n  # Disable the axis and plot\n  plt.axis('off')\n  display(plt.show())\n  \n  return (graph, csv)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["We can now call our function to visualise our respective graphs.\n\n### 6.1.1. Internal Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data = {}\ngraph_data[\"internal_requests\"] = plot_visualisation(sorted_itemsets[\"internal_requests\"])"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### 6.1.2. External Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"external_requests\"] = plot_visualisation(sorted_itemsets[\"external_requests\"])"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### 6.1.3. Hong Kong Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"hk_requests\"] = plot_visualisation(sorted_itemsets[\"hk_requests\"])"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### 6.1.4. USA Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"us_requests\"] = plot_visualisation(sorted_itemsets[\"us_requests\"])"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### 6.1.5. Australian Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"au_requests\"] = plot_visualisation(sorted_itemsets[\"au_requests\"])"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### 6.1.6. PC Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"pc_requests\"] = plot_visualisation(sorted_itemsets[\"pc_requests\"])"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### 6.1.7. Smartphone Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"smartphone_requests\"] = plot_visualisation(sorted_itemsets[\"smartphone_requests\"])"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["### 6.1.8. Tablet Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"tablet_requests\"] = plot_visualisation(sorted_itemsets[\"tablet_requests\"])"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["### 6.1.9. Bot Requests"],"metadata":{}},{"cell_type":"code","source":["graph_data[\"bots_requests\"] = plot_visualisation(sorted_itemsets[\"bots_requests\"])"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["## 6.2. Improved plotting using GraphViz\n\nHowever, the above is hard to read, especially the frequency values. We can convert the graph into a [Graphviz](http://www.graphviz.org) diagram string. Install the dependency as needed:\n\n```\n$ brew install graphviz\n```\n\nRunning the command below, we can copy the output and run through the `dot` command provided by Graphviz:\n\n```\n$ pbpaste > a2.dot\n$ dot internal_requests.dot -T pdf > internal_requests.pdf\n```"],"metadata":{}},{"cell_type":"code","source":["def graph_to_pydot_string(graph, layout=\"dot\"):\n  \"\"\" Converts the graph to a representable Graphviz diagram using pydot\n  \n  Args:\n      graph (`NetworkX.DiGraph`): The graph to convert\n  \n  Returns:\n      string: A string representing the Graphviz diagram string with the \n              layout specified, defaults to `dot`.\n  \"\"\"\n  string = nx.drawing.nx_pydot.to_pydot(graph).to_string()\n  # Split all lines to add the specified layout\n  lines = string.split(\"\\n\")\n  lines.insert(1, 'layout=\"%s\";' % layout)\n  return \"\\n\".join(lines)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["### 6.2.1. Internal Requests\n\nNow run our conversion function on our graph:"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"internal_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["Save the output above to file `internal_requests.dot` and convert using the commands described above.\n\nThis produces a much cleaner looking output, where edges are colorised for assisting with reading frequency patterns between pages.\n\n![Dot output](https://i.imgur.com/hbxRIYj.png)"],"metadata":{}},{"cell_type":"markdown","source":["### 6.2.2. External Requests\n\n![Dot image](https://i.imgur.com/3wco2n9.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"external_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["### 6.2.3. Hong Kong Requests\n\n![Dot image](https://i.imgur.com/7KoDjBd.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"hk_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["### 6.2.4. USA Requests\n\n![Dot image](https://i.imgur.com/8qvRlq1.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"us_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["### 6.2.5. Australian Requests\n\n![Dot image](https://i.imgur.com/4uvAdI9.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"au_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["### 6.2.6. PC Requests\n\n![Dot image](https://i.imgur.com/3jPUfDZ.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"pc_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["### 6.2.7. Smartphone Requests\n\n![Dot image](https://i.imgur.com/Ne5cq0l.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"smartphone_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["### 6.2.8. Tablet Requests\n\n![Dot image](https://i.imgur.com/kWvmh2l.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"tablet_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["### 6.2.9. Bot Requests\n\n![Bot requests](https://i.imgur.com/RZB4Ab7.png)"],"metadata":{}},{"cell_type":"code","source":["print graph_to_pydot_string(graph_data[\"bots_requests\"][0])"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["## 7. CSV Output\n\nWe can provide a reference table to support the diagram as a table and import this into our LaTeX report:"],"metadata":{}},{"cell_type":"code","source":["print \"internal_requests.csv\"\nprint graph_data[\"internal_requests\"][1]\nprint\nprint \"external_requests.csv\"\nprint graph_data[\"external_requests\"][1]\nprint\nprint \"hk_requests.csv\"\nprint graph_data[\"hk_requests\"][1]\nprint\nprint \"us_requests.csv\"\nprint graph_data[\"us_requests\"][1]\nprint\nprint \"au_requests.csv\"\nprint graph_data[\"au_requests\"][1]\nprint\nprint \"pc_requests.csv\"\nprint graph_data[\"pc_requests\"][1]\nprint\nprint \"smartphone_requests.csv\"\nprint graph_data[\"smartphone_requests\"][1]\nprint\nprint \"tablet_requests.csv\"\nprint graph_data[\"tablet_requests\"][1]\nprint\nprint \"bots_requests.csv\"\nprint graph_data[\"bots_requests\"][1]"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["print \"internal_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"internal_requests\"][0])\nprint\nprint \"external_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"external_requests\"][0])\nprint\nprint \"hk_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"hk_requests\"][0])\nprint\nprint \"us_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"us_requests\"][0])\nprint\nprint \"au_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"au_requests\"][0])\nprint\nprint \"pc_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"pc_requests\"][0])\nprint\nprint \"smartphone_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"smartphone_requests\"][0])\nprint\nprint \"tablet_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"tablet_requests\"][0])\nprint\nprint \"bots_requests.dot\"\nprint graph_to_pydot_string(graph_data[\"bots_requests\"][0])\nprint"],"metadata":{},"outputs":[],"execution_count":81}],"metadata":{"name":"DataAnalytics","notebookId":980727703319900},"nbformat":4,"nbformat_minor":0}
