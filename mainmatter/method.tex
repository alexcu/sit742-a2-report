\section{Analysis Method}
\label{sec:method}

Within this section, a detailed summary is given about how the information found in Section~\ref{sec:results} were collated together. We define the assumptions that were made in order to extrapolate the data and refer to the data extraction and mining processes used to find these patterns.

\subsection{Assumptions}
\label{sec:method:assumptions}

\subsubsection{User Sessions}
\label{sec:method:assumptions:user_sessions}

A \textit{web session} is defined as a single session of a particular user at a given moment of time. It consists of what pages were visited within that period of time, as determined by the requests made to the server in this period.

To extrapolate meaningful data, the definition of \textit{what} a user session was needed to be determined. This is because we need to find patterns made in one particular user session; when a user visits the website in one sitting, what pages do they visit and in what order?

To do this, we assume that a session is made up of the following factors within a series of web requests:

\begin{enumerate}
  \item The client's IP address must be the same,
  \item The client's user agent string must be the same,
  \item The timestamp of the session is grouped in the same day, and
  \item The timestamp of the session is within the same hour.
\end{enumerate}

We group every request using these four key factors, using a hash as a delimiter, into the field \texttt{session\_identifier}. Using this identifier, we can group up a series of requests into one particular session. For example, a sample request is made to the server:

\begin{itemize}
  \item The \texttt{c\_ip} field is \texttt{1.2.3.4},
  \item The \texttt{user\_agent} field is \texttt{Safari},
  \item The \texttt{timestamp} field is \texttt{Tue Feb 29 03:18:00 GMT 2017}
\end{itemize}

Therefore the \texttt{session\_identifier} would be:

\begin{lstlisting}
      1.2.3.4#Safari#29-02-2017#03
\end{lstlisting}

Hence, multiple requests made within the 3rd hour of the 29th of February 2017 by the IP address \texttt{1.2.3.4} with the user agent \texttt{Safari} will be gathered together. Obviously this example is trivial and is more detailed in practice, but as an explanation it helps to be simple.

Advantages of this approach is that there can be multiple requests made from a single IP address that \textit{are not} from the same user agent. For example, internal requests made by users within the hotel would use different computers, and therefore different \texttt{user\_agent} strings would be made, making the session identifier unique.

Disadvantages of this approach are that there could be two users on an internal network (i.e., same \texttt{client\_ip}) using the \textit{exact} same user agent within the same exact hour who are technically not the same person. Additionally, by grouping sessions by the hour, any session that was near the hour may be split into two sessions. These are some limitations of our approach.

\subsubsection{Selection of Resources}
\label{sec:method:assumptions:selection_of_resources}

Not all requests were included in the analysis, as some requests were simply resource requests. We want to focus purely on \textit{informational resources}, and therefore need to filter the number of requests to those that are relevant.

To do this, we consider the following:

\begin{enumerate}
  \item The request resource is non-multimedia or functional, but informational. That is, requests whose resources are not JavaScript, Cascading Style Sheets, Images and the like, but rather ASPX and ASHX files (ASP.NET and Generic Web Handler pages that contain information about the hotel).
  \item The request resource is not under the directories \texttt{media}, \texttt{layouts}, or \texttt{sitecore}, as these directories do not contain informational resources.
  \item The request returns a client status that is only 200 (Success), or is not a placeholder page for a status page, e.g., a \texttt{404.aspx} page would not be appropriate to consider.
\end{enumerate}

Additionally, we make all request resources consistent for comparison by making them all lower case, which is made visible in the diagrams shown in Section~\ref{sec:results}.

\subsection{Extraction Process}
\label{sec:method:extraction_process}

\subsubsection{Data Mapping}
\label{sec:method:extraction_process:data_mapping}

Following the process made in Assignment 1, all publicly known client IP addresses were extracted from the MaxMind GeoIP2\footnote{See \url{http://dev.maxmind.com/geoip/geoip2/}.} dataset to determine request locations. This therefore allowed us to retrieve sessions from the top three countries, that being: (1) Hong Kong; (2) the USA; and (3) Australia. Determination that these were the top three countries can be found in Assignment 1.

Additionally, user agent strings were parsed to analyse device and browser statistics using the Python \texttt{user-agents} library\footnote{See \url{https://pypi.python.org/pypi/user-agents}.} These user agent strings allowed determination of whether or not users were using: (1) PCs; (2) Smartphones; (3) Tablets; or (4) if they were actually Bots crawling the website.

\subsubsection{IP Address Source Regular Expression}
\label{sec:method:extraction_process:ip_regex}

To differentiate between private site visitors and external visitors, a regular expression was used to filter the private and public IP address ranges. The regular expression is shown below:

\begin{lstlisting}[language=SQL, xleftmargin=2cm]
WHERE l.c_ip REGEXP 
	'(^127\.)|(^10\.) | 
	 (^172\.1[6-9]\.) |
	 (^172\.2[0-9]\.) |
	 (^172\.3[0-1]\.) |
	 (^192\.168\.)'
\end{lstlisting}

Negating this \texttt{WHERE} clause of the regular expression will select only public IP addresses.

\subsection{Data Mining}

Data analysis was gathered using Frequent Pattern mining of user sessions. The Apache Spark implementation \citep{spark} uses FPGrowth, an algorithm described by \citet{Han:2000:MFP:335191.335372} that calculates item frequencies to identify frequent items in a given set of data. The provided data in our case was the extracted data from our SQL queries which were developed inside the attached Python Notebook.

The Apache Spark implementation requires the data parameter \texttt{minSupport}, which is the the threshold for an itemset to be identified as frequent. The default support level chosen was 0.01 (i.e., if a frequent item appears 10 times out of 1000 transactions, then its support is 0.01).

However, this default support value was massaged and relaxed in some instances where few patterns could be identified. These cases are: Hong Kong, Smartphone and Tablet requests, where the support level was relaxed by 50\%; and Australian requests, where the support level was relaxed by 75\%.

Additionally, we wish to retrieve sessions that contain \textit{three or more} requests in the session. This is to ensure there are multiple hits in the session, and to retrieve more meaningful patterns.